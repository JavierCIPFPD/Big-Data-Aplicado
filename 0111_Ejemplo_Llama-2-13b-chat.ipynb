{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JavierCIPFPD/Big-Data-Aplicado/blob/main/0111_Ejemplo_Llama-2-13b-chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 2\n",
        "\n",
        "\n",
        "Meta, en colaboración con Microsoft, ha acaba de lanzar una bomba Lama 2 un montón de nuevos modelos lingüísticos de código abierto este lanzamiento es una gran noticia.\n",
        "\n",
        "Llama 2 es el modelo lingüístico de código abierto de código abierto de última generación potencia de fuego: se ha entrenado 40% más de datos que su predecesor predecesor, llama 1, y se ha duplicado del contexto se ha duplicado.\n",
        "\n",
        "La parte jugosa llama 2 viene en tres diferentes tamaños un modelo de 7 mil millones, un modelo de 13 mil millones de parámetros y un modelo de 70 mil millones y además todos han sido pre-entrenados en la asombrosa cantidad de 2 trillones de tokens con una longitud de contexto de 4.000 tokens, lo que significa que estos modelos están sólo un paso por detrás de Chat GPT.\n",
        "\n",
        "Aquí tienes su web de descarga: https://ai.meta.com/llama/\n",
        "\n",
        "Ahora lo vamos a ejecutar en local en nuestra máquina de colab, es super sencillo:\n",
        "\n"
      ],
      "metadata": {
        "id": "cqHFjrXVBJcn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCFOzsQSHbjM"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "!git clone -b v2.5 https://github.com/camenduru/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/generation_config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o generation_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/model-00001-of-00002.safetensors -d /content/text-generation-webui/models/code-llama-instruct-7b -o model-00001-of-00002.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/model-00002-of-00002.safetensors -d /content/text-generation-webui/models/code-llama-instruct-7b -o model-00002-of-00002.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/model.safetensors.index.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o model.safetensors.index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/special_tokens_map.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/tokenizer.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/tokenizer.model -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer.model\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/tokenizer_config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer_config.json\n",
        "\n",
        "!echo \"dark_theme: true\" > /content/settings.yaml\n",
        "!echo \"chat_style: wpp\" >> /content/settings.yaml\n",
        "\n",
        "%cd /content/text-generation-webui\n",
        "!python server.py --share --settings /content/settings.yaml --model /content/text-generation-webui/models/code-llama-instruct-7b"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}